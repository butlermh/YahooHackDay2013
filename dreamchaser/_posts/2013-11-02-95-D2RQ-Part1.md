---
layout: post
title: Making data available on the Semantic Web
description: "Making data available on the Semantic Web"
modified: 2013-11-02
category: articles
comments: true
author: mark
---

[Tim Berners-Lee](http://en.wikipedia.org/wiki/Tim_Berners-Lee), the creator of the World Wide Web, has proposed a vision of how the web could evolve into the [Semantic Web](http://en.wikipedia.org/wiki/Semantic_web).

Both [Linked Data](http://en.wikipedia.org/wiki/Linked_data) and [Open Data](http://en.wikipedia.org/wiki/Open_data) are influenced by and related to the idea of the Semantic Web. In essence, Open Data is primarily about making data available to everyone, so they can re-use it new ways. Linked data is about joining that data up, so that where [possible schemas are mapped](http://en.wikipedia.org/wiki/Schema_mapping), and linking together data that refers to the same entity. For example, we might have several different datasets that refer to Taipei Main Station or Taipei 101.

There is an [overlap between the Semantic Web and Linked Data](http://tomheath.com/blog/2009/03/linked-data-web-of-data-semantic-web-wtf/), and the Semantic Web work predates Linked Data, but one difference is the Semantic Web typically involves richer ways of modelling data, such as ontologies.

The Semantic Web uses a standardized data representation called [RDF](http://en.wikipedia.org/wiki/Resource_Description_Framework) and uses a standard query language called SPARQL. RDF comes in several different serializations: one, [RDF/XML](http://www.w3.org/TR/rdf-syntax-grammar/), is based on [XML](http://en.wikipedia.org/wiki/Xml). Another, [Turtle/N3](http://www.w3.org/TR/turtle/), is superficially similar to JSON, although RDF describes a graph whereas JSON describes a tree. RDF also supports [namespaces](http://en.wikipedia.org/wiki/Namespaces) which allows different vocabularies to be combined in a modular way, and hence different data sources to be integrated more easily. In a similar way, SPARQL has some similarities to [YQL](http://developer.yahoo.com/yql/), a technology developed by Yahoo, although YQL can process a wider range of data sources. 

So we have lots of open data sources - but how do we put them on the Semantic Web? In this example we will describe how we can leverage some readily available technologies to make our open data available as RDF and provide a SPARQL end point to query it. The approach we will use is as follows: first we will convert the JSON to CSV format. Then we will ingest the CSV data into a relational database. Then we will use [D2RQ](http://d2rq.org/) to map the data to RDF and provide a SPARQL endpoint.

First though, let's get the [restaurant dataset](http://data.taipei.gov.tw/opendata/apply/NewDataContent;jsessionid=C1BACB4D704764015BFC9B5EF899B2D0?oid=74A57445-2312-4E95-9128-7833590D3D77) we used earlier: 

{% highlight bash %}
wget http://data.taipei.gov.tw/opendata/apply/json/NzRBNTc0NDUtMjMxMi00RTk1LTkxMjgtNzgzMzU5MEQzRDc3 -o cutlery.json  
{% endhighlight %} 

Next to convert it to CSV we create a simple Python script: 

{% highlight python %}
#!/usr/bin/env python   
# -*- coding: utf-8 -*-  
import unicodecsv, json, sys, codecs  
input = codecs.open(sys.argv[1], 'r', encoding='utf8')  
data = json.load(input)  
input.close()  
sys.stdout = codecs.getwriter('utf8')  
file = open(sys.argv[2], 'w')  
output = unicodecsv.writer(file)  
output.writerow(data[0].keys()) # header row  
for row in data:  
output.writerow(row.values())  
{% endhighlight %} 

The script uses [unicodecsv](https://github.com/jdunck/python-unicodecsv) - a unicode friendly version of the Python library for dealing with CSV files - so you need to install this using your favourite Python package manager: 

{% highlight bash %}
pip install unicodecsv  
{% endhighlight %} 

or if you prefer Easy Install:

{% highlight bash %}
easy_install unicodecsv  
{% endhighlight %} 

Then you run `json2csv` as follows: 

{% highlight bash %}
./json2csv cutlery.json cutlery.csv  
{% endhighlight %} 

If you do you have MySQL installed you can install it like this: 

{% highlight bash %}
sudo apt-get install mysql-server
{% endhighlight %}   

Next to load the CSV data into MySQL, we need to modify the MySQL security settings slightly. The is the hardest part of this hack. On Ubuntu, MySQL uses [AppArmor](http://en.wikipedia.org/wiki/AppArmor) to sandbox MySQL and provide better security. It took a little while to work out how to set the security permissions so we could load the CSV file into MySQL. To do this open `usr.sbin.mysqld`:

{% highlight bash %}
sudo vi /etc/apparmor.d/usr.sbin.mysqld  
{% endhighlight %}   

then add add these two lines so MySQL can load files from `/tmp`: 

{% highlight bash %}
/tmp/ rw,  
/tmp/* rw,  
{% endhighlight %}   

Save the file, quite vi, then restart AppArmor and MySQL: 

{% highlight bash %}
sudo /etc/init.d/apparmor restart  
service mysql restart 
{% endhighlight %}  

Now copy the CSV file to `/tmp`: 

{% highlight bash %}
cp cutlery.csv /tmp
{% endhighlight %}   

Then log into MySQL: 

{% highlight bash %}
mysql -u user
{% endhighlight %}   

Now create a database to store the data. It is critical to set the character set correctly as the data contains traditional Chinese. 

{% highlight sql %}
CREATE DATABASE data_taipei_gov_tw DEFAULT CHARSET=utf8 DEFAULT COLLATE utf8_general_ci;  
USE data_taipei_gov_tw;  
SET NAMES utf8;  
{% endhighlight %} 

Next create the table that will store the data: 

{% highlight sql %}
CREATE TABLE RestaurantsWithCutleryDiscount   
(Y FLOAT,  
name VARCHAR(30),  
district VARCHAR(30),  
url VARCHAR(40),  
photo VARCHAR(40),  
telephone VARCHAR(10),  
longitude FLOAT,  
operationType VARCHAR(20),  
X FLOAT,  
latitude FLOAT,  
discountCategory VARCHAR(20),  
operationCategory VARCHAR(20),  
address VARCHAR(30),  
discountPeriod VARCHAR(20),  
openingHours VARCHAR(30),  
discountContent VARCHAR(20),  
discountCondition VARCHAR(20),  
id INT NOT NULL AUTO_INCREMENT PRIMARY KEY);
{% endhighlight %}   

Then load the data from the CSV file into this table: 

{% highlight sql %}
LOAD DATA INFILE '/tmp/cutlery.csv' INTO TABLE RestaurantsWithCutleryDiscount FIELDS TERMINATED BY ',' IGNORE 1 LINES;
{% endhighlight %}  

Then it is a good idea to check that the data reached the table correctly: 

{% highlight sql %}
select * from RestaurantsWithCutleryDiscount limit 10;  
{% endhighlight %}

Great! So we have got the JSON data into a relational database - although there is a limitation here - this will not work for all JSON datasets as JSON is a hierarchical data format. Luckily this particular data set is flat, so it is possible to convert it to CSV and then import it into MySQL. The next step is to install MySQL. MySQL relies on Java so if you do not use Java regularly check if it is installed: 

{% highlight bash %}
$java -version             
java version "1.7.0_10"  
Java(TM) SE Runtime Environment (build 1.7.0_10-b18)  
Java HotSpot(TM) 64-Bit Server VM (build 23.6-b04, mixed mode)  
{% endhighlight %}

If Java is not installed, you can install it on Ubuntu like this: 

{% highlight bash %}
sudo add-apt-repository ppa:webupd8team/java  
sudo apt-get update  
sudo apt-get install oracle-jdk7-installer 
{% endhighlight %} 

Then download D2RQ: 

{% highlight bash %}
wget https://github.com/downloads/d2rq/d2rq/d2rq-0.8.1.tar.gz  
{% endhighlight %}

and unzip it to your local apps directory (create one if it does not already exist): 

{% highlight bash %}
mkdir ~/apps  
cd apps  
wget https://github.com/downloads/d2rq/d2rq/d2rq-0.8.1.tar.gz  
tar -xvzf d2rq-0.8.1.tar.gz  
rm d2rq-0.8.1.tar.gz
{% endhighlight %}  

Then you will need to get a JDBC (Java Database Connector) for MySQL and copy it to the right place in D2RQ: 

{% highlight bash %}
wget http://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.26.tar.gz/from/http://cdn.mysql.com/ --output-document jdbc.tar.gz  
tar -xvzf jdbc.tar.gz  
cp mysql-connector-java-5.1.26/mysql-connector-java-5.1.26-bin.jar ~/apps/d2rq-0.8.1/lib  
rm jdbc.tar.gz  
{% endhighlight %}

Next use D2RQ to generate a mapping to map the database to RDF: 

{% highlight bash %}
cd d2rq-0.8.1 
./generate-mapping -o mapping.ttl -d com.mysql.jdbc.Driver -u user -p password jdbc:mysql://localhost/data_taipei_gov_tw
{% endhighlight %}

Then start the D2RQ server: 

{% highlight bash %}
./d2r-server mapping.ttl -b http://yahoo.clouduct.com:2222/
{% endhighlight %}

Then you can [browse the dataset via the D2RQ user interface](http://yahoo.clouduct.com:2020/) - navigate to this URL and click on the link next to Home to explore the data set. 

{% highlight bash %}
http://localhost:2020/  
{% endhighlight %}
Alternatively you can use SNORQL that provides a SPARQL sandbox to explore the dataset - although you will need to learn some SPARQL! 

{% highlight bash %}
http://localhost:2020/snorql/  
{% endhighlight %}
SNORQL also lets you browse the data via properties 

{% highlight bash %}
http://localhost:2020/snorql/?browse=properties  
{% endhighlight %}
or via classes 

{% highlight bash %}
http://localhost:2020/snorql/?browse=classes  
{% endhighlight %}

There are some important issues we have not covered like mapping the schema to a standard vocabulary, and linking common instances to other data sets, but hey - the data is now available in RDF and queryable via SPARQL which is a start!
